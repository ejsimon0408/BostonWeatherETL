{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T17:20:49.045982Z",
     "start_time": "2025-12-08T17:20:49.012416Z"
    }
   },
   "source": [
    "from prefect import flow, task, get_run_logger\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=5, log_prints=True)\n",
    "def extract_from_api(lat: float = 42.3601, lon: float = -71.0589) -> dict:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Fetching live weather data for lat={lat}, lon={lon}...\")\n",
    "\n",
    "    url = (\n",
    "        f\"https://api.open-meteo.com/v1/forecast?\"\n",
    "        f\"latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        current = response.json().get(\"current_weather\", {})\n",
    "\n",
    "        if \"time\" in current:\n",
    "            dt = pd.to_datetime(current[\"time\"]).tz_localize(\"UTC\").tz_convert(\"America/New_York\").tz_localize(None)\n",
    "            current[\"parsed_time\"] = dt\n",
    "\n",
    "        logger.info(f\"Fetched API: {current.get('temperature')}Â°C at {current.get('parsed_time')}\")\n",
    "        return current\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"API request failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def extract_from_s3_parquet(s3_bucket: str, s3_prefix: str) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Reading from s3://{s3_bucket}/{s3_prefix}\")\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "\n",
    "    parquet_files = []\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            if obj['Key'].endswith('.parquet'):\n",
    "                parquet_files.append(obj['Key'])\n",
    "\n",
    "    logger.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for file_key in parquet_files:\n",
    "        year = month = None\n",
    "        for part in file_key.split('/'):\n",
    "            if part.startswith('year='):\n",
    "                try:\n",
    "                    year = int(part.split('=')[1])\n",
    "                except:\n",
    "                    year = None\n",
    "            elif part.startswith('month='):\n",
    "                try:\n",
    "                    month = int(part.split('=')[1])\n",
    "                except:\n",
    "                    month = None\n",
    "\n",
    "        obj = s3.get_object(Bucket=s3_bucket, Key=file_key)\n",
    "        df = pd.read_parquet(BytesIO(obj['Body'].read()))\n",
    "\n",
    "        if 'year' not in df.columns or df['year'].isna().all():\n",
    "            df[\"year\"] = year\n",
    "        if 'month' not in df.columns or df['month'].isna().all():\n",
    "            df[\"month\"] = month\n",
    "\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Total rows loaded from S3: {len(combined_df)}\")\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def normalize_parquet_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Normalizing columns for {len(df)} rows\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.warning(\"Input df is empty\")\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in [\"day\", \"year\", \"month\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if \"datatype\" in df.columns and \"value\" in df.columns:\n",
    "        mask = df[\"value\"].notna() & df[\"datatype\"].notna()\n",
    "        if mask.any():\n",
    "            legacy = df[mask].copy()\n",
    "            legacy[\"element\"] = legacy[\"datatype\"]\n",
    "            legacy[\"value_c\"] = (legacy[\"value\"] / 10 - 32) * 5 / 9\n",
    "            df.loc[mask, \"element\"] = legacy.loc[mask.index, \"element\"]\n",
    "            df.loc[mask, \"value_c\"] = legacy.loc[mask.index, \"value_c\"]\n",
    "\n",
    "    wide_value_cols = [c for c in [\"TMAX\", \"TMIN\", \"PRCP\"] if c in df.columns]\n",
    "    wide_rows_mask = df[\"value_c\"].isna() & df[wide_value_cols].notna().any(axis=1) if wide_value_cols else pd.Series(False, index=df.index)\n",
    "\n",
    "    long_rows_mask = df[\"element\"].notna() & df[\"value_c\"].notna()\n",
    "    long_rows = df[long_rows_mask].copy()\n",
    "    wide_rows = df[~long_rows_mask].copy()\n",
    "\n",
    "    melted_rows = pd.DataFrame()\n",
    "    if wide_value_cols:\n",
    "        id_cols = [c for c in wide_rows.columns if c not in wide_value_cols]\n",
    "        if not wide_rows.empty:\n",
    "            try:\n",
    "                melted = wide_rows.melt(id_vars=id_cols, value_vars=wide_value_cols,\n",
    "                                        var_name='element', value_name='value_c')\n",
    "                melted = melted[melted['value_c'].notna()].copy()\n",
    "                melted_rows = melted\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to melt wide rows: {e}\")\n",
    "                melted_rows = pd.DataFrame()\n",
    "\n",
    "    if not long_rows.empty and not melted_rows.empty:\n",
    "        df_combined = pd.concat([long_rows, melted_rows], ignore_index=True, sort=False)\n",
    "    elif not long_rows.empty:\n",
    "        df_combined = long_rows.reset_index(drop=True)\n",
    "    elif not melted_rows.empty:\n",
    "        df_combined = melted_rows.reset_index(drop=True)\n",
    "    else:\n",
    "        df_combined = df.copy()\n",
    "\n",
    "    if 'windspeed' not in df_combined.columns:\n",
    "        df_combined['windspeed'] = np.nan\n",
    "\n",
    "    for col in ['year', 'month', 'day']:\n",
    "        if col not in df_combined.columns:\n",
    "            df_combined[col] = pd.NA\n",
    "        df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
    "\n",
    "    if 'element' not in df_combined.columns:\n",
    "        df_combined['element'] = pd.NA\n",
    "    if 'value_c' not in df_combined.columns:\n",
    "        df_combined['value_c'] = pd.NA\n",
    "\n",
    "    logger.info(f\"Normalized data rows: {len(df_combined)}\")\n",
    "    logger.info(f\"Years in combined data: {sorted(pd.Series(df_combined['year'].dropna().unique()).tolist())}\")\n",
    "    logger.info(f\"Unique elements: {pd.Series(df_combined['element'].dropna().unique()).tolist()}\")\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def compute_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    if df.empty:\n",
    "        logger.warning(\"compute_flags received empty dataframe\")\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\")\n",
    "    df[\"day\"] = pd.to_numeric(df[\"day\"], errors=\"coerce\")\n",
    "    df[\"value_c\"] = pd.to_numeric(df[\"value_c\"], errors=\"coerce\")\n",
    "\n",
    "    df_valid = df.dropna(subset=[\"year\", \"month\", \"day\", \"value_c\", \"element\"]).copy()\n",
    "    logger.info(f\"Rows with valid year/month/day/value_c/element: {len(df_valid)}\")\n",
    "\n",
    "    if df_valid.empty:\n",
    "        logger.warning(\"After filtering, no valid rows remain.\")\n",
    "        return df_valid\n",
    "\n",
    "    df_valid[\"date\"] = pd.to_datetime(\n",
    "        df_valid[[\"year\", \"month\", \"day\"]].astype(int),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df_valid = df_valid.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "    threshold = 3\n",
    "\n",
    "    tmax_df = df_valid[df_valid[\"element\"] == \"TMAX\"].copy()\n",
    "\n",
    "    if tmax_df.empty:\n",
    "        logger.warning(\"No TMAX rows found for flag computation\")\n",
    "        return df_valid\n",
    "\n",
    "    tmax_daily_mean = (\n",
    "        tmax_df.groupby([\"month\", \"day\"])[\"value_c\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"value_c\": \"daily_mean\"})\n",
    "    )\n",
    "    tmax_df = tmax_df.merge(tmax_daily_mean, on=[\"month\", \"day\"], how=\"left\")\n",
    "\n",
    "    tmax_monthly_mean = (\n",
    "        tmax_df.groupby(\"month\")[\"value_c\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"value_c\": \"monthly_mean\"})\n",
    "    )\n",
    "    tmax_df = tmax_df.merge(tmax_monthly_mean, on=\"month\", how=\"left\")\n",
    "\n",
    "    tmax_df[\"daily_flag\"] = np.where(\n",
    "        tmax_df[\"value_c\"] < tmax_df[\"daily_mean\"] - threshold, \"Below\",\n",
    "        np.where(tmax_df[\"value_c\"] > tmax_df[\"daily_mean\"] + threshold, \"Above\", \"Average\")\n",
    "    )\n",
    "\n",
    "    tmax_df[\"monthly_flag\"] = np.where(\n",
    "        tmax_df[\"value_c\"] < tmax_df[\"monthly_mean\"] - threshold, \"Below\",\n",
    "        np.where(tmax_df[\"value_c\"] > tmax_df[\"monthly_mean\"] + threshold, \"Above\", \"Average\")\n",
    "    )\n",
    "\n",
    "    df_valid = df_valid.merge(\n",
    "        tmax_df[[\"date\", \"daily_flag\", \"monthly_flag\"]],\n",
    "        on=\"date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    if 'windspeed' not in df_valid.columns:\n",
    "        df_valid['windspeed'] = np.nan\n",
    "\n",
    "    return df_valid\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def transform_api_data(api_data: dict) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    if \"temperature\" not in api_data or \"parsed_time\" not in api_data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dt = api_data[\"parsed_time\"]\n",
    "    df = pd.DataFrame([{\n",
    "        \"year\": dt.year,\n",
    "        \"month\": dt.month,\n",
    "        \"day\": dt.day,\n",
    "        \"date\": dt,\n",
    "        \"value_c\": api_data[\"temperature\"],\n",
    "        \"TMAX\": api_data[\"temperature\"],\n",
    "        \"windspeed\": api_data.get(\"windspeed\", np.nan),\n",
    "        \"element\": \"TMAX\",\n",
    "        \"source\": \"api\"\n",
    "    }])\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def compute_api_flags(historical_df: pd.DataFrame, api_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    if api_df.empty:\n",
    "        logger.warning(\"API dataframe is empty, skipping flag computation\")\n",
    "        return api_df\n",
    "\n",
    "    if historical_df.empty:\n",
    "        logger.warning(\"Historical dataframe empty, cannot compute flags\")\n",
    "        api_df[\"daily_flag\"] = np.nan\n",
    "        api_df[\"monthly_flag\"] = np.nan\n",
    "        return api_df\n",
    "\n",
    "    if 'date' not in api_df.columns:\n",
    "        api_df['date'] = pd.to_datetime(api_df[['year', 'month', 'day']])\n",
    "\n",
    "    threshold = 3\n",
    "\n",
    "    daily_mean = (\n",
    "        historical_df[historical_df['element']=='TMAX']\n",
    "        .groupby(['month', 'day'])['value_c']\n",
    "        .mean()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    monthly_mean = (\n",
    "        historical_df[historical_df['element']=='TMAX']\n",
    "        .groupby(['month'])['value_c']\n",
    "        .mean()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    def daily_flag(row):\n",
    "        avg = daily_mean.get((row['month'], row['day']), np.nan)\n",
    "        if pd.isna(avg):\n",
    "            return np.nan\n",
    "        if row['TMAX'] > avg + threshold:\n",
    "            return \"Above\"\n",
    "        elif row['TMAX'] < avg - threshold:\n",
    "            return \"Below\"\n",
    "        else:\n",
    "            return \"Average\"\n",
    "\n",
    "    def monthly_flag(row):\n",
    "        avg = monthly_mean.get(row['month'], np.nan)\n",
    "        if pd.isna(avg):\n",
    "            return np.nan\n",
    "        if row['TMAX'] > avg + threshold:\n",
    "            return \"Above\"\n",
    "        elif row['TMAX'] < avg - threshold:\n",
    "            return \"Below\"\n",
    "        else:\n",
    "            return \"Average\"\n",
    "\n",
    "    api_df['daily_flag'] = api_df.apply(daily_flag, axis=1)\n",
    "    api_df['monthly_flag'] = api_df.apply(monthly_flag, axis=1)\n",
    "    return api_df\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def combine_and_pivot(file_df: pd.DataFrame, api_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    if file_df.empty:\n",
    "        pivot_df = pd.DataFrame()\n",
    "        logger.warning(\"file_df is empty\")\n",
    "    else:\n",
    "        key_elements = ['TMAX', 'TMIN', 'PRCP']\n",
    "        file_df = file_df[file_df['element'].isin(key_elements)].copy()\n",
    "\n",
    "        pivot_df = file_df.pivot_table(\n",
    "            index=[\"date\", \"year\", \"month\", \"day\"],\n",
    "            columns=\"element\",\n",
    "            values=\"value_c\",\n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "\n",
    "        for col in [\"daily_flag\", \"monthly_flag\", \"windspeed\"]:\n",
    "            if col in file_df.columns:\n",
    "                series = file_df.groupby(\"date\")[col].first()\n",
    "                pivot_df[col] = pivot_df['date'].map(series)\n",
    "\n",
    "    if not api_df.empty:\n",
    "        pivot_df = pd.concat([pivot_df, api_df], ignore_index=True, sort=False)\n",
    "\n",
    "    logger.info(f\"Final combined dataframe rows: {len(pivot_df)}\")\n",
    "    return pivot_df\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def upload_to_s3_csv(df: pd.DataFrame, bucket: str, key: str):\n",
    "    logger = get_run_logger()\n",
    "    from io import StringIO\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "    logger.info(f\"Uploaded CSV to s3://{bucket}/{key}\")\n",
    "\n",
    "\n",
    "@flow(name=\"boston-weather-pipeline\", log_prints=True)\n",
    "def boston_weather_pipeline(\n",
    "    s3_bucket=\"weather-etl-emily-2025\",\n",
    "    s3_parquet_prefix=\"processed/raw/\",\n",
    "    s3_output_bucket=\"weather-etl-emily-2025\",\n",
    "    s3_output_key=\"combined/boston_weather_combined.csv\"\n",
    "):\n",
    "    api_data = extract_from_api()\n",
    "    boston_df = extract_from_s3_parquet(s3_bucket, s3_parquet_prefix)\n",
    "    normalized_df = normalize_parquet_columns(boston_df)\n",
    "    flagged_df = compute_flags(normalized_df)\n",
    "    processed_api = transform_api_data(api_data)\n",
    "    processed_api = compute_api_flags(flagged_df, processed_api)\n",
    "    final_df = combine_and_pivot(flagged_df, processed_api)\n",
    "    upload_to_s3_csv(final_df, s3_output_bucket, s3_output_key)\n",
    "\n",
    "    return {\"records\": len(final_df), \"csv_s3_path\": f\"s3://{s3_output_bucket}/{s3_output_key}\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = boston_weather_pipeline()\n",
    "    print(result)"
   ],
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m<tokenize>:16\u001B[0;36m\u001B[0m\n\u001B[0;31m    )\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
